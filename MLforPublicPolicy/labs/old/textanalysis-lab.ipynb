{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis Lab\n",
    "---\n",
    "\n",
    "In this lab, we are going to analyze job advertisements from 2010-2015 using topic modeling to examine the content of our data and document classification to tag the type of job in the advertisement. First we will go over how to transform our data into a matrix that can be read in by an algorithm. \n",
    " \n",
    "\n",
    "\n",
    "## Glossary of Terms\n",
    "\n",
    "* **Corpus**: A corpus of documents is the set of all documents in the dataset.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is sepearated into meaningful terms or phrases. In english this is fairly triial as words as separated by whitespace. \n",
    "\n",
    "* **Stemming**: Stemming is a type of text normalization where words that have different forms but their essential meaning at the same are normalized to the original dictionary form of a word. For example \"go,\" \"went,\" and \"goes\" all stem from the lemma \"go.\"\n",
    "\n",
    "* **TFIDF**: TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole.\n",
    "\n",
    "* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of co-occuring words are clustered into topics. Typically, the words in a cluster should be related and make sense (e.g, boat, ship, captain). Individual documents will then fall into multiple topics. \n",
    "\n",
    "* **LDA**: LDA (latent Dirichlet allocation) is a type of probabilistic model commonly used for topic modelling. \n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning like prepositions, articles and common nouns. They can often be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "import nltk\n",
    "import ujson\n",
    "import re\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords') #download the latest stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Our Dataset for this tutorial will be a subset job descriptions data from 2010-2015 compiled by the Commonwealth of Virginia. it is stored in the file jobs_subset_data.csv in this repository (in the data directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load this data into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our table has 4 fields. `normalizedTitle_onetName`, `normalizedTitle_onetCode`, `jobDescription`, `title`\n",
    "\n",
    "Onet is an online database that contains hundreds of occupational definitions. https://en.wikipedia.org/wiki/Occupational_Information_Network\n",
    "\n",
    "The normalizedTitle_onetName and the normalizedTitle_onetCode are derived from the Onet Database. We wil use the names in the document tagging portion of the tutorial. The jobDescription is the actual jobDescription and the title is derived from the jobDescription. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique job titles are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each job description has a great deal of information contained in unstructered text. We can use text analysis to find overarching concepts that are in our corpus.  This will allow us to the discover the most important words and phrases in the job descriptions and give us a big-picture of the content in our collection. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling (clustering)\n",
    "\n",
    " We are going to apply topic modeling, an unsuperivised learning method, to our corpus to find the high-level topics in our corpus as a first-go for exploring our data. As we apply topic modeling we will discuss ways of cleaning and preprocessing our data to get the best results.\n",
    "\n",
    "Topic modeling is a broad subfield of machine learning and natural language processing. We are going to focus on one approach, Latent Dirichlet allocation (LDA). LDA is a fully Bayesian extension of probabilistic latent semantic  indexing, itself a probabilistic extension of latent semantic analysis.\n",
    "\n",
    "In topic modeling we first assume the existence of topics in the corpus and that there is a small number of topics that can explain a corpus. Topics, in this case, are a ranked-list of words from our corpus, with the highest probability words at the top. A single document can be explained by multiple topics. For instance, an article on net neutrality has to do with both technology and politics. The set of topics used by a document is known as the document's allocation, hence, the name latent Dirchlet allocation, each document has an allocation of latent topics allocated by Dirchlet distribution. \n",
    "\n",
    "## Processing text data\n",
    "\n",
    "The first important step in working with text data is cleaning and processing the data. This include but is not limited to *forming a corpus of text, tokenization, removing stop-words, finding words colocated together (N-grams), and stemming and lemmatization*. Each of these steps will be discussed below. The ultimate goal is to transform our text data into a form an algorithm can work with. A sequence of symbols cannot be fed directly into an algorithm. Algorithms expect numerical feature vectors with fixed size rather then a document with a variable document length. We will be transforming our text corpus into a *bag of n-grams* to be further analyzed. In this form our text data is represented as a matrix where each row refers to a specific job description (document) and each column is the occurence of a word (feature).\n",
    "\n",
    "\n",
    "### Bag of n-gram representation example\n",
    "\n",
    "Ultimately, we want to take our collection of documents, corpus, and convert it into a matrix. Fortunately sklearn has a pre-built object, `CountVectorizer`, that can tokenize, eliminate stopwords, identify n-grams and stem our corpus, outputing a matrix in one step. Before we apply the vectorizer on our corpus of data we are going to apply it to a toy example so we can understand the output and how a bag of words is represented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_words(corpus,\n",
    "                       NGRAM_RANGE=(0,1),\n",
    "                       stop_words = None,\n",
    "                        stem = False,\n",
    "                       MIN_DF = 0.05,\n",
    "                       MAX_DF = 0.95,\n",
    "                       USE_IDF=False):\n",
    "    \"\"\"\n",
    "    Turn a corpus of text into a bag-of-words.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: ls\n",
    "        test of documents in corpus    \n",
    "    NGRAM_RANGE: tuple\n",
    "        range of N-gram. Default (0,1)\n",
    "    stop_words: ls\n",
    "        list of commonly occuring words that have little semantic\n",
    "        value\n",
    "    stem: bool\n",
    "        use a stemmer to stem words\n",
    "    MIN_DF: float\n",
    "       exclude words that have a frequency less than the threshold\n",
    "    MAX_DF: float\n",
    "        exclude words that have a frequency greater than the threshold\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bag_of_words: scipy sparse matrix\n",
    "        scipy sparse matrix of text\n",
    "    features:\n",
    "        ls of words\n",
    "    \"\"\"\n",
    "    #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "    if stem:\n",
    "        tokenize = lambda x: [stemmer.stem(i) for i in x.split()]\n",
    "    else:\n",
    "        tokenize = None\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                tokenizer=tokenize, \n",
    "                                ngram_range=NGRAM_RANGE,\n",
    "                                stop_words = stop_words,\n",
    "                                strip_accents=STRIP_ACCENTS,\n",
    "                                min_df = MIN_DF,\n",
    "                                max_df = MAX_DF)\n",
    "    \n",
    "    bag_of_words = vectorizer.fit_transform( corpus ) #transform our corpus is a bag of words \n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    if USE_IDF:\n",
    "        NORM = None #turn on normalization flag\n",
    "        SMOOTH_IDF = True #prvents division by zero errors\n",
    "        SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "        transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "        #get the bag-of-words from the vectorizer and\n",
    "        #then use TFIDF to limit the tokens found throughout the text \n",
    "        tfidf = transformer.fit_transform(bag_of_words)\n",
    "        \n",
    "        return tfidf, features\n",
    "    else:\n",
    "        return bag_of_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_corpus = ['this is document one', 'this is document two', 'text analysis on documents is fun'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_bag_of_words, toy_features = create_bag_of_words(toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counter_vectorizer outputs a matrix. In this case a sparse matrix, a matrix that has a lot more 0s then 1s. To save space scipy has special methods for storing sparse matrices in a space-efficient way rather than saving many many 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_bag_of_words = toy_bag_of_words.toarray()\n",
    "np_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has been transformed into a 3x9 matrix where each row corresponds to a document and the columns correspond to the features. A 1 indicates the existence of the feature or word in the document, 0 indicates the word is not present.Our toy corpus is now ready to be analyzed. We illustrated bag of n-gram with a toy example because the matrix for a much larger corpus would be much larger and harder to interpet on our corpus of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word counts\n",
    "\n",
    "As a initial look into the data we can examine what the top few words are in our corpus. We can sum the columns of the bag_of_words and then convert to a numpy array. From here we can zip the features and word_count into a dictionary\n",
    "and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_counts(bag_of_words, feature_names):\n",
    "    \"\"\"\n",
    "    Get the ordered word counts from a bag_of_words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bag_of_words: obj\n",
    "        scipy sparse matrix from CounterVectorizer\n",
    "    feature_names: ls\n",
    "        list of words\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts: dict\n",
    "        Dictionary of word counts\n",
    "    \"\"\"\n",
    "    np_bag_of_words = bag_of_words.toarray()\n",
    "    word_count = np.sum(np_bag_of_words,axis=0)\n",
    "    np_word_count = np.asarray(word_count).ravel()\n",
    "    dict_word_counts = dict( zip(feature_names, np_word_count) )\n",
    "    \n",
    "    orddict_word_counts = OrderedDict( \n",
    "                                    sorted(dict_word_counts.items(), key=lambda x: x[1], reverse=True), )\n",
    "    \n",
    "    return orddict_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_counts(toy_bag_of_words, toy_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's repeat this with our jobs data\n",
    "\n",
    "### Create our Corpus\n",
    "\n",
    "First we need to form our corpus, a set of multiple similiar documents. In our case, our corpus is the set of all job descriptions. We can pull out the job descriptions from the data frame by pulling out the underlying numpy array using the `.values` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column with the job descriptions from the data frame you loaded earlier\n",
    "# code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_topics(tfidf, features, N_TOPICS=3, N_TOP_WORDS=5,):\n",
    "    \"\"\"\n",
    "    Given a matrix of features of text data generate topics\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    tfidf: scipy sparse matrix\n",
    "        sparse matrix of text features\n",
    "    N_TOPICS: int\n",
    "        number of topics (default 10)\n",
    "    N_TOP_WORDS: int\n",
    "        number of top words to display in each topic (default 10)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ls_keywords: ls\n",
    "        list of keywords for each topics\n",
    "    doctopic: array\n",
    "        numpy array with percentages of topic that fit each category\n",
    "    N_TOPICS: int\n",
    "        number of assumed topics\n",
    "    N_TOP_WORDS: int\n",
    "        Number of top words in a given topic. \n",
    "    \"\"\"\n",
    "    \n",
    "    i=0\n",
    "    lda = LatentDirichletAllocation( n_topics= N_TOPICS,\n",
    "                                       learning_method='online') #create an object that will create 5 topics\n",
    "    i+=1\n",
    "    doctopic = lda.fit_transform( tfidf )\n",
    "    i+=1\n",
    "    \n",
    "    ls_keywords = []\n",
    "    for i,topic in enumerate(lda.components_):\n",
    "        word_idx = np.argsort(topic)[::-1][:N_TOP_WORDS]\n",
    "        keywords = ', '.join( features[i] for i in word_idx)\n",
    "        ls_keywords.append(keywords)\n",
    "        print(i, keywords)\n",
    "        i+=1\n",
    "            \n",
    "    return ls_keywords, doctopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_bag_of_words, corpus_features = create_bag_of_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first aspect of the feature list that should stand out for us is that the first few entries are numbers that have no real semantic meaning. There are also other useless words such as prepositions and articles that also have no semantic meaning. The words *ability* or *abilities* or *accuracy* and *accurate* are also quite similiar and mean the same thing. We should try cleaning our corpus of data of these types of words as they just add noise to our analysis. Nevertheless let's try creating topics and seeing the quality of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_counts(corpus_bag_of_words, corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our top words are articles, prepositions and conjunctions that do not tell us anything about our courpus. Let's march on create topics anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_corpus_keywords, corpus_doctopic = create_topics(corpus_bag_of_words, corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these topics we have no real knowledge of what is in our corpus, with the exception that there are job ads written in Spanish. The problem is the the top words in the topics are conjunctions and prepositions that have no semantic information. We have to try and clean and process our data to get more meaningful infomation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "\n",
    "To clean and normalize text we will remove all special characters, numbers, and punctuation. Then we will make all the text lowercase to normalize the text;  this is so words like \"the\" and \"The\" will be counted as the same in our analysis. To remove the special characters, numbers and punctuation we will use regular expressions. \n",
    "\n",
    "#### Regular Expressions\n",
    "\n",
    ">\"Some people, when confronted with a problem, think \n",
    ">'I know, I'll use regular expressions.'   Now they have two problems.\"\n",
    "> -- Jaime Zawinski\n",
    "\n",
    "Regular Expressions or regexes match a certain amount text in a document based on a set of rules and syntax. The name \"regular expressions\" actually comes from the mathematical theory it is based on. These rules are useful for pulling out useful information in a large amount of text (e.g., email addresses, html-tags, credit card numbers). Regexes often match text much more quickly then plain text sorting and can often reduce their development time. Some regular expressions can become quite complicated and it may then become a good idea to write code using Python. Any developer should keep in mind there is a trade-off between optimization and understandibility. In Python, a general philosophy is code is meant to be as understandable by *people* as much as possible. You should therefore always tend toward the understandabilty side of things rather than overly optimizing your code. Your future-self, code-reviewers, people who inherit your code, and anyone else who has to make sense of your code in the future will appreciate it. \n",
    "\n",
    "For our purposes we are going to use a regular expression to match all characters that are not letters -- punctutation, quotes, special characters and numbers --replace them with spaces and then take all the remaining characters and make them lowercase. \n",
    "\n",
    "A full tutorial on regular expressions would be outside the scope of this tutorial. There are many good tutorials that can be found on-line. There is also a great interactive tool for developing and checking regular expressions regex101.com. \n",
    "\n",
    "We will be using the `re` library in python for regular expression matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of the punctuations and set all characters to lowercase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's check one sample row before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first description after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it all  lowercase? Are all numbers and special chracters removed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Now that we have cleaned our text we can tokenize it by deciding which terms and phrases are the most meaningful. In this case we want to split our text into individual words. Our words are separted by spaces so we can use the `.split()` command to turn are document into a list of words using a space as the character to split on as an example. Normally the `CountVectorizer` handles this for us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = processed_corpus[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are words that have very little semantic meaning and are found throughout a text. Having the word *the* or *of* will tell us nothing about our corpus, nor will they be meaningful features.  Examples of stopwords are prepositions, articles and common nouns. To process the corpus even further we can eliminate these stopwords by checking if the are in a list of commonly occuring stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords =  stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample of stopwords\n",
    "eng_stopwords[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,stop_words=eng_stopwords)\n",
    "dict_processed_word_counts = get_word_counts(processed_bag_of_words, processed_features)\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now let's see how this affects the topics that are produce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate topics/clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these clusters mean? do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's increase the number of clusters to 5 and call crate_topics again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase clusters to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the clusters turn out to be different now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's increase number of clusters to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase clusters to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a good amount of topics. Some of the top words may be quite similiar such as engineering and enginner. We can reduce those words to its stem to further refine our features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmitzation\n",
    "\n",
    "We can further process our text through *stemming and lemmatization*. Words can take on muliple forms with limited change to their meaning. For example \"systems\", \"systematic\" and \"system\" are all different words but they all have the same meeting. We can replace all these words with system witout losing any meaning. The lemma is the original dictionary form of a word (e.g. lying and lie). There are several well known stemming algorithms -- Porter, Snowball, Lancaster -- that all have strengths and weakneses. For this tutorial we are using the Porter Stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemmer.stem('lies'))\n",
    "print(stemmer.stem(\"lying\"))\n",
    "print(stemmer.stem('systematic'))\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True)\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not it appears we have picked up some extra topics that describe the educational requirements for a job ad or the equal opportunity clause of a job ad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "Individual words are not always the the correct unit of analysis. Prematurely removing stopwords can lead to losing pharases such as \"kick the bucket\", \"commander in chief\", or \"sleeps with the fishes\". Idenitfying these N-grams requires looking for patterns of words that often appear together in fixed patterns. \n",
    "\n",
    "Now let's transform our corpus into a bag of n-grams which in this case is a bag of bi-grams or bag of 2-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True,\n",
    "                                                                 NGRAM_RANGE=(0,2))\n",
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice one of the top words in one of the topics is \"northrop grumman\", a bi-gram!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF (Term Frequency Inverse Document Frequency)\n",
    "\n",
    "A final step in cleaning and processing our text data is TFIDF. TFIDF (Term frequency-inverse document frequency) is an example of feature enginnering where the most important words are extracted by taking account their frequency in documents and the entire corpus of documents as a whole. Words that appear in all documents are deemphazized while more meaningful words are emphaized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus,\n",
    "                                                                 stop_words=domain_specific_stopwords,\n",
    "                                                                 stem=True,\n",
    "                                                                 NGRAM_RANGE=(0,2),\n",
    "                                                                 USE_IDF = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_word_counts = get_word_counts(processed_bag_of_words,\n",
    "                   processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words counts have been reweighted to emphasize the more meaninful words of the corpus while deemphasizing the the are found throughout the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords, processed_doctopic = create_topics(processed_bag_of_words, \n",
    "                                                       processed_features, \n",
    "                                                      N_TOPICS = 10,\n",
    "                                                      N_TOP_WORDS= 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grab the topic_id of the majority topic for each document and store it in a list\n",
    "ls_topic_id = [np.argsort(processed_doctopic[comment_id])[::-1][0] for comment_id in range(len(corpus))]\n",
    "df_jobs_data['topic_id'] = ls_topic_id #add to the dataframe so we can compare with the job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each row is tagged with a topic id let's see how well the topics explain the job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num = 1\n",
    "print(processed_keywords[topic_num])\n",
    "df_jobs_data[ df_jobs_data.topic_id == topic_num ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now explore other cluster numbers and see what they mean? do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore other clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Document Classification.\n",
    "\n",
    "Now we turn our attention to supervised learning. Previously, using topic modelling, we were inferring relationships between the data. In supervised learning, we produce a label, *y*, given some data *x*. In order to produce labels we need to first have examples our algorithm can learn from, a training set. Developing a training set can be very expensive, as it can require a large amount of human labor or linguistic expertise. Document classification is the case where our x are documents and our y are what the documents are (e.g, title for a job position). A common example of document classification is spam detection in emails. In sentiment analysis our x is our documents and y is the state of the author. This can range from an author being happy or unhappy with a product or the author being pollitically conservative or liberal. There is also Part-of-speech tagging were our x are individual words and y is the part-of-speech. \n",
    "\n",
    "In this section we are going to train a classifier to classify job titles using our jobs dataset. We will use two job categories ('Credit Analysts' and 'Financial Examiners') and build a model to distinguish between them.\n",
    "\n",
    "Here's what we want you to do:\n",
    "\n",
    "1. Load the training data set\n",
    "2. Load the test data set\n",
    "3. Clean and Pre-Process them\n",
    "4. Generate Features\n",
    "5. Build model using training set\n",
    "6. Evaluate model on test set\n",
    "7. explore the model you just built\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train_corpus.csv')\n",
    "df_test = pd.read_csv('./data/test_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['normalizedTitle_onetName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df_train['normalizedTitle_onetName'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['normalizedTitle_onetName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df_test['normalizedTitle_onetName'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is job advertisements for credit analysts and financial examiners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process our Data\n",
    "\n",
    "In order to feed our data into a classifier we need to pull out the labels, our y's, and a clean corpus of documents, x, for our training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training set  - you need to clean the text data and create two arrays (words and labels)\n",
    "# create train_corpus\n",
    "\n",
    "\n",
    "# create test set  - you need to clean the text data and create two arrays (words and labels)\n",
    "# create test_corpus\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we had done in the unsupervised learning we have to transform our data. This time we have to transform our testing and training set into two different bag-of-words. The classifer will learn from the training set and we will evaluate the clasffier's performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters for vectorizer \n",
    "ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode'\n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) #Range for pharases of words\n",
    "MIN_DF = 0.01 # Exclude words that have a frequency less than the threshold\n",
    "MAX_DF = 0.8  # Exclude words that have a frequency greater then the threshold \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer=None, # alternatively tokenize_and_stem but it will be slower \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = None #turn on normalization flag\n",
    "SMOOTH_IDF = True #prvents division by zero errors\n",
    "SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "USE_IDF = True #flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "\n",
    "#get the bag-of-words from the vectorizer and\n",
    "#then use TFIDF to limit the tokens found throughout the text \n",
    "start_time = time.time()\n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) #using all the data on for generating features!! Bad!\n",
    "test_bag_of_words = vectorizer.transform( test_corpus )\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "features = vectorizer.get_feature_names()\n",
    "print('Time Elapsed: {0:.2f}s'.format(\n",
    "        time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot pass the label \"Credit Analyst\" or \"Financial Examiner\" into the classifier. Instead we to encode them as 0s and 1s using the labelencoder as a part of sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#relabel our labels as a 0 or 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create arrays of indices so we can access the training and testing sets accoringly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = df_train.shape[0]\n",
    "train_set_idx = np.arange(0,train_size)\n",
    "test_set_idx = np.arange(train_size, len(labels))\n",
    "train_labels_binary = labels_binary[train_set_idx]\n",
    "test_labels_binary = labels_binary[test_set_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can fit a classifier using the training set and predict scores on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #train the classifer to get the model\n",
    " # get scores of documents in the test set as belonging to Credit or Financial Analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls\n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " plot_precision_recall_n(labels_binary[test_set_idx], y_score[:,1], 'LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine our precision-recall curve we can see that our precision is 1 and recall is 0.8 up to 0.4 percent of the population. Unlike the previous example where we are using a precision at k curve to prioritize our resources. We can still use a precision at k curve to see what parts of the corpus can be tagged by the classifier and which should undergo a manual clerical review. Based on this we can make decisions of what documents should be manually tagged by a person during a clerical rewiew, say, the percent of the population above 40%. \n",
    "\n",
    "Alternatively, we can try to maximize the entire precision-recall space. In this case we need a different metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score[:,1])\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(labels_binary[test_set_idx],y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the area under the curve, 0.96, we see we have a very good classifier. The AUC shows how accurate our scores are under different cut-off thresholds. If you recall from the Machine Learning tutorial, the model outputs a score. We then set a cutoff to bin each score as a 0 or 1. The closer our scores are to the true values the more resilent they are to different cutoffs. For instance, if our scores were perfect our AUC would be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_feature_importances_for_logit(coef,features, labels, num_features=10):\n",
    "    \"\"\"\n",
    "    output feature importances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coef: numpy\n",
    "        feature importances\n",
    "    features: ls \n",
    "        feature names\n",
    "    labels: ls\n",
    "        labels for the classifier\n",
    "    num_features: int\n",
    "        number of features to output (default 10)\n",
    "    \n",
    "    Example\n",
    "    --------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    coef = mdl.coef_.ravel()\n",
    "\n",
    "    dict_feature_importances = dict( zip(features, coef) )\n",
    "    orddict_feature_importances = OrderedDict( \n",
    "                                    sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "\n",
    "    ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "    num_features = 10\n",
    "    label0_features = ls_sorted_features[:num_features] \n",
    "    label1_features = ls_sorted_features[-num_features:] \n",
    "\n",
    "    print(labels[0],label0_features)\n",
    "    print(labels[1], label1_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_feature_importances(mdl.coef_.ravel(), features, ['Credit Analysts','Financial Examiner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances are which words are the most relevant for predicting the type of Job Ad. We would expect words like credit, customer and candidate to be found in ads for a Credit Analyst. While words like review officer, compliance would be found in ads for a Financial Examiner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great resource for NLP in python is \n",
    "[Natural Language Processing with Python](https://www.amazon.com/Natural-Language-Processing-Python-Analyzing/dp/0596516495)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
