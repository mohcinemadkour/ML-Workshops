{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Classification with Logistic Regression and SVM\n",
    "In this lab we will get some more practice with classification methods(Logistic Regression and Support Vector Machines), cross validation.\n",
    "\n",
    "As in the previous lab we will be working with projects.csv and outcomes.csv from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "proj = pd.read_csv('/local/hopan/data/projects.csv')\n",
    "outcomes = pd.read_csv('/local/hopan/data/outcomes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Cleaning the dataset\n",
    "\n",
    "As you have probably noticed, this dataset has a mix of continuous features and categorical features(encoded as strings).\n",
    "Before we can apply any of our classification algorithms to predict the is_exciting variable, we first need our desired features in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of proj: 664098\n",
      "Length of outcomes: 664098\n",
      "Count of proj.project_id: 664098\n",
      "Count of outcomes.project_id: 619326\n",
      "Count of outcomes.is_exciting: 619326\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of proj: %d\" %len(proj))\n",
    "print(\"Length of outcomes: %d\" %len(proj))\n",
    "print(\"Count of proj.project_id: %d\" %proj.projectid.count())\n",
    "print(\"Count of outcomes.project_id: %d\" %outcomes.projectid.count())\n",
    "print(\"Count of outcomes.is_exciting: %d\" %outcomes.is_exciting.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join on project id\n",
    "all_data = pd.merge(proj, outcomes, how='inner', left_on='projectid', right_on='projectid')\n",
    "y_values = all_data['is_exciting'].astype(np.bool)\n",
    "all_x_values = all_data[proj.columns]\n",
    "\n",
    "# pick the features you want to use\n",
    "features = []\n",
    "x_values = all_x_values[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix datatypes\n",
    "You'll notice that a ton of the columns are binary or true/false columns. But all of them are encoded as 't'/'f' strings. If we want to feed these variables into any of the sklearn classifiers, we need to fix this. Convert all of the binary columns to booleans. Hint: use the astype, or apply functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_bools(df, bool_cols):\n",
    "    bool_cols = []\n",
    "    for col in bool_cols:\n",
    "        df[col] = df[col].astype(np.bool)\n",
    "        # Alternatively\n",
    "        # proj[col] = proj[col].apply(np.bool)\n",
    "\n",
    "# apply it to the dataframe\n",
    "bool_cols = []\n",
    "convert_bools(x_values, bool_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize categorical features\n",
    "Most of the non-numerical/non-id columns are strings but they really exist as a discrete set of categories. For example: the \"poverty_level\" column takes on 4 values: \"low poverty\", \"moderate poverty\", \"high poverty\", and \"highest poverty.\"  To convert these columns into a form that we can feed into our classifiers, we can need to convert this categorical variable to binary variables.\n",
    "\n",
    "For each of the categorical features you want to add to the classifier, convert the column into a set of binary/indicator columns. The pandas function [get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) might be helpful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binarize_categories(df, cat_cols, drop=True):\n",
    "    '''\n",
    "    df: a pandas dataframe\n",
    "    cat_cols: list of column names to generate indicator columns for\n",
    "    drop: a bool. If true, drop the original category columns\n",
    "    Returns: the modified dataframe\n",
    "    '''\n",
    "    for col in cat_cols:\n",
    "        binary_cols = pd.get_dummies(df[col], col)\n",
    "        df = pd.merge(df, binary_cols, left_index=True, right_index=True, how='inner')\n",
    "    if drop:\n",
    "        df.drop(cat_cols, inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>a_5</th>\n",
       "      <th>a_6</th>\n",
       "      <th>a_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    b  a_5  a_6  a_7\n",
       "10  0    1    0    0\n",
       "13  1    0    1    0\n",
       "14  3    0    0    1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy test example before running it on the real dataframe\n",
    "d = pd.DataFrame({'a': [5,6,7], 'b':[0,1,3]}, index=[10,13,14])\n",
    "d = binarize_categories(d, ['a'], True)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply it to the real dataframe\n",
    "cat_cols = []\n",
    "cleaned_x_values = binarize_categories(x_values, cat_cols, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4% missing from: Column school_ncesid\n",
      "0.0% missing from: Column school_zip\n",
      "12.3% missing from: Column school_metro\n",
      "0.1% missing from: Column school_district\n",
      "0.0% missing from: Column school_county\n",
      "0.0% missing from: Column teacher_prefix\n",
      "0.0% missing from: Column primary_focus_subject\n",
      "0.0% missing from: Column primary_focus_area\n",
      "31.3% missing from: Column secondary_focus_subject\n",
      "31.3% missing from: Column secondary_focus_area\n",
      "0.0% missing from: Column resource_type\n",
      "0.0% missing from: Column grade_level\n",
      "5.3% missing from: Column fulfillment_labor_materials\n",
      "0.0% missing from: Column students_reached\n"
     ]
    }
   ],
   "source": [
    "# Check the proportion of missing values from columns\n",
    "for c in proj.columns:\n",
    "    if proj[c].count() < len(proj):\n",
    "        missing_perc = ((len(proj) - proj[c].count()) / float(len(proj))) * 100.0\n",
    "        print(\"%.1f%% missing from: Column %s\" %(missing_perc, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decent chunk of the columns are missing values. If you're using features that are missing values, you need to do something to take care of them(IE: drop those rows, fill in the missing value, add a new category that corresponds to missing value, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop rows\n",
    "x_values = cleaned_x_values.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we've done a ton of cleaning and it can be hard to keep track of all this stuff\n",
    "# in the future, it would be good to just have a clean_data function that takes care of all this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data \n",
    "Split the data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 80/20 train test split. But you can tweak the test size\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.20, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "You'll notice that LogisticRegression takes a ton of parameters. We'll play around with the \"penalty\" and \"C\" parameters.\n",
    "If we set the penalty parameter to ['l2'](http://mathworld.wolfram.com/L2-Norm.html), sklearn's LogisticRegression model solves the following minimization problem:\n",
    "\n",
    "$$ \\min_{\\beta} ||\\beta||_2 + C \\sum_{i} \\log ( -y_i (X_i^T \\beta) +1)$$\n",
    "\n",
    "Similarly, if we set the penalty parameter to ['l1'](http://mathworld.wolfram.com/L2-Norm.html), LogisticRegression will solve the following minimization problem:\n",
    "\n",
    "$$\\min_{\\beta} ||\\beta||_1 + C \\sum_{i} \\log ( -y_i (X_i^T \\beta) +1)$$\n",
    "\n",
    "where $$||\\beta||_2 = \\sqrt { \\sum_{i} \\beta_i^2 }$$ and $$||\\beta||_1 =  \\sum_{i} | \\beta_i | $$ \n",
    "\n",
    "Try running logistic regression with both L1 and L2 penalties and a mix of C values with K-fold cross validation on the training set. If the K-fold cross validation takes too long, consider using a smaller subset of the data. Write a function to perform the cross validation over both penalties, and a range of C values. Hint: recall the \"wonderful for loop\" from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cv_logistic_regression(x_values, y_values, c_vals, folds=5):\n",
    "    # Keep track of both the accuracy scores and the model generated\n",
    "    scores = {'l1': {}, 'l2': {}}\n",
    "    models = {'l1': {}, 'l2': {}}\n",
    "    kf = KFold(folds)\n",
    "    \n",
    "    for train_indices, test_indices in kf.split(df):\n",
    "        x_train, x_test = x_values[train_indices], x_values[test_indices]\n",
    "        y_train, y_test = y_values[train_indices], y_values[test_indices]\n",
    "        for lpenalty in ['l1', 'l2']:\n",
    "            for c in c_vals:\n",
    "                lr = LogisticRegression(penalty=lpenalty, C=c)\n",
    "                lr.fit(x_train, y_train)\n",
    "                y_predict = lr.predict(x_test)\n",
    "                scores[lpenalty][c] = accuracy_score(y_test, y_predict)\n",
    "                print(\"%s with C = %.2f score: %.2f\" %(lpenalty, c, scores[lpenalty][c]))\n",
    "    return scores, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What combination of parameters gives the highest cross validation score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_values = [10**i for i in range(-2, 3)]\n",
    "scores, models = cv_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really see the difference between L1 and L2 regularization, we need to take a closer look at the models they produced. Plot a histogram of the weight values of LogisticRegression models for each C value. You can access these weight coefficients via the coef\\_ attribute in LogisticRegression. Do you notice anything interesting happening as the C value varies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot histogram of weights for l1 and l2 models\n",
    "num_rows = len(c_values)\n",
    "num_cols = 2\n",
    "plt.subplot()\n",
    "index = 1\n",
    "for penalty in ('l1', 'l2'):\n",
    "    for c in c_values:\n",
    "        plt.subplot(num_rows, num_cols, index)\n",
    "        plt.hist(models[penalty][c].coef_, bins=30)\n",
    "        plt.title('Logistic Regression with %s penalty, C = %.2f' %(penalty, c))\n",
    "        plt.xlabel('Weight values')\n",
    "        plt.ylabel('Frequency')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the number of weight coefficients set to 0 in L1 regularized Logistic Regression increases with smaller C values. Revisit the minimization problems: you can think of the \"C\" parameter as a way of specifying the tradeoff between the L1/L2 penalty and the negative log likelihood of the model. A small value of \"C\" means that we are assigning greater weight to the L1/L2 penalty(equivalently downweighting the negative log likelihood).\n",
    "L1 regularization induces sparse models - this can be a very useful if you suspect your classification target variable can be explained by few features.\n",
    "\n",
    "Now try normalizing your continuous features. Rerun the logistic regression cross validation. Does normalization change the performance of Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_cols(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = (df[col] - np.mean(df[col])) / np.std(df[col])\n",
    "    return df\n",
    "\n",
    "norm_cols = []\n",
    "x_values = normalize_cols(x_values, norm_cols)\n",
    "# rerun the cv_logistic_regression function\n",
    "\n",
    "# remember that any data transformations you apply on the training set must be applied on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Applying SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "SVC also takes quite a few parameters. We will be playing with following parameters:\n",
    "* C\n",
    "* kernel\n",
    "\n",
    "SVM tries to find the hyperplane that maximizes the \"margin\" between the two classes of points. The \"C\" parameter in [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) has the same role as the \"C\" parameter in LogisticRegression: it tells you how much to penalize the \"size\" of the weight vector. Note that SVC only allows for L2 regularization.\n",
    "\n",
    "## Choosing the kernel\n",
    "\n",
    "\n",
    "As we did for LogisticRegression, write a function/some code to perform KFold crossvalidation on the training set to find the optimal C and kernel parameters. Repeat the cross validation after normalizing your continous features as well. Does normalization affect the SVM performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def cv_svm(x_values, y_values, c_vals, kernels, folds):\n",
    "    # Keep track of both the accuracy scores and the model generated\n",
    "    scores = {k: {} for k in kernels}\n",
    "    models = {k: {} for k in kernels}\n",
    "    kf = KFold(folds)\n",
    "    \n",
    "    for train_indices, test_indices in kf.split(df):\n",
    "        x_train, x_test = x_values[train_indices], x_values[test_indices]\n",
    "        y_train, y_test = y_values[train_indices], y_values[test_indices]\n",
    "        for k in kernels:\n",
    "            for c in c_vals:\n",
    "                svc = SVC(C=c, kernel=k)\n",
    "                svc.fit(x_train, y_train)\n",
    "                y_predict = lr.predict(x_test)\n",
    "                scores[lpenalty][c] = accuracy_score(y_test, y_predict)\n",
    "                print(\"%s kernel with C = %.2f score: %.2f\" %(k, c, scores[k][c]))\n",
    "    return scores, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Class Imbalance and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in lecture, accuracy is not necessarily the best way to evaluate your classifier. This is especially true in situations where\n",
    "our classification dataset has class imbalance. In our current dataset, we have the following class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting proportion: 0.06\n",
      "Not exciting proportion: 0.94\n"
     ]
    }
   ],
   "source": [
    "exciting_prop = len(outcomes.is_exciting[outcomes.is_exciting == 't']) / float(len(outcomes.is_exciting))\n",
    "print(\"Exciting proportion: %.2f\" %exciting_prop)\n",
    "print(\"Not exciting proportion: %.2f\" % (1 - exciting_prop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these proportions, a trivial classifier that simply outputs \"Not exciting\" no matter what the input features are will have\n",
    "an accuracy of 94%. So try changing your cross validation functions above to optimize for precision, recall, F1 score, any any other [classification metrics](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) you've seen in lecture. Hint: make the classification scoring function a parameter to your cross validation function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use grid search cv: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both LogisticRegression and SVC have a class_weight parameter that allows you to specify how much the model should prefer correctly classifying one class over another. Try specifying this parameter in your models and see how this affects the evaluation metrics you just tried above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Steps\n",
    "If you have the time, here are some additional things to try:\n",
    "* pull in other features from the other csv files from the Kaggle\n",
    "* try adding transformed features into the models(IE: squared values, products of features, etc)\n",
    "* you've written a cv_logistic_regression function and a cv_svm function which probably look very similar. Try writing a cross validation function that takes a model(IE: LogisiticRegression, SVC, DecisionTreeClassifier) as a parameter as well so that we can just do cv_model(SVC, ....) and cv_model(LogisticRegression, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
