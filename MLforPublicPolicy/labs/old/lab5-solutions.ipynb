{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Ensemble Methods: Bagging, Boosting, Random Forests\n",
    "\n",
    "In this lab you will get familiar with ensemble methods. We will cover bagging, boosting (AdaBoost) and random forests in the exercises in this lab. Please refer to \n",
    "http://scikit-learn.org/stable/modules/ensemble.html#adaboost for an introduction to these methods and some example usages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Import and cleaning\n",
    "We will use the same data as in the previous two labs (Kaggle KDD Cup 2014) so you can use the same cleaned data for\n",
    "this lab session. In order to save some time you can also use the following parts taken from the solution to the\n",
    "previous lab and modify as you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "proj = pd.read_csv('data/projects.csv')\n",
    "outcomes = pd.read_csv('data/outcomes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join on project id\n",
    "all_data = pd.merge(proj, outcomes, how='inner', left_on='projectid', right_on='projectid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pick the columns you want to use\n",
    "# include 'is_exciting' column here'\n",
    "main_cols = [ 'teacher_prefix', 'primary_focus_area',\n",
    "            'poverty_level', 'total_price_excluding_optional_support', 'total_price_including_optional_support',\n",
    "           'students_reached','eligible_double_your_impact_match', 'is_exciting']\n",
    "\n",
    "main_variables = all_data[main_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_bools(df):\n",
    "    d = {'t': True, 'f': False}\n",
    "    return df.replace(d)\n",
    "\n",
    "    \n",
    "# apply it to the dataframe\n",
    "main_variables = convert_bools(main_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize_categories(df, cat_cols, drop=True):\n",
    "    '''\n",
    "    df: a pandas dataframe\n",
    "    cat_cols: list of column names to generate indicator columns for\n",
    "    drop: a bool. If true, drop the original category columns\n",
    "    Returns: the modified dataframe\n",
    "    '''\n",
    "    for col in cat_cols:\n",
    "        binary_cols = pd.get_dummies(df[col], col)\n",
    "        df = pd.merge(df, binary_cols, left_index=True, right_index=True, how='inner')\n",
    "    if drop:\n",
    "        df.drop(cat_cols, inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# apply it to the real dataframe\n",
    "cat_cols = [  'teacher_prefix', 'primary_focus_area','poverty_level']\n",
    "cleaned_main_variables = binarize_categories(main_variables, cat_cols, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop rows with NaN\n",
    "main_variables = cleaned_main_variables.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# separate the features and the outcomes\n",
    "y_values = main_variables['is_exciting']\n",
    "del main_variables['is_exciting']\n",
    "x_values = main_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 80/20 train test split. But you can tweak the test size\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.20, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Bagging\n",
    "You can use bagging to make a stronger estimator from simple base estimators. Bagging merges independent estimators which are made using different random subsets of the training samples. Refer to \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#r154 \n",
    "\n",
    "to check the parameters. This function provides more than just bagging, for instance in addition to you can also take random subsets of the features (max_features). In order to implement bagging, you need to keep all the features but use random subsets of the samples. You can use the bootstrap parameter to specify that your samples are drawn with replacement. Use n_estimators and max_samples to specify the number of the estimators you want to use and the number of samples you want to use for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bagging with Decision Tree\n",
    "Use Decision Tree as  your base classifier. You can start with depth 20 for your decision trees. Since the data\n",
    "is very unbalanced regarding to the number of True and False samples, use the class_weight parameter to specify\n",
    "how much the model should prefer correctly classifying one class over another.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "lr = DecisionTreeClassifier(max_depth = 20 , class_weight = 'balanced')\n",
    "bagging = BaggingClassifier( lr, n_estimators = 5, max_samples=0.65, max_features=1.)\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "score = bagging.score(X_test, y_test)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###What happens when you try different bagging parameters?\n",
    "Try n_estimator = { 5 ,10, 20 } , max_depth = { 10, 20} and max_samples = { 0.35, 0.5, 0.65 }\n",
    "and report the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bagging with logistic regression\n",
    "Use logistic regression as  your base classifier. To keep it simple use l2 norm and C = 1. Since the data is very unbalanced regarding to the number of True and False samples, use the class_weight parameter to specify how much the model should prefer correctly classifying one class over another.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424396585835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression('l2', C=1, class_weight = 'balanced')\n",
    "bagging = BaggingClassifier( lr, n_estimators = 10, max_samples=0.65, max_features=1.)\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "score = bagging.score(X_test, y_test)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###What happens when you try different bagging parameters?\n",
    "Try n_estimator = { 5 ,10, 20 } and max_samples = { 0.35, 0.5, 0.65 } and report the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Boosting - AdaBoost\n",
    "\n",
    "Another approach for making stronger estimators from the basic ones is boosting. In contrast to bagging, boosting makes a strong classifier by adding the features one by ones based on the predictive power. At each step of boosting training samples are re-weighted to give a higher weight to the ones which were wrongly classified and direct the algorithm to choose features which are useful for classifying those samples.\n",
    "\n",
    "In this part you will make a classifier using AdaBoost which as a popular boosting algorithm. Refer to\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "for the specifications. The default estimator is decision tree classifier but you can use any estimator of your choice as far as it has the conditions of the base estimator for AdaBoost. For instance since AdaBoost changes the weights of the samples, your base classifier should support this propoerty. Here again, you need to specify the number of simple classifiers by n_estimators.\n",
    "\n",
    "#### Make sure you have enough features\n",
    "\n",
    "For using AdaBoost since the base classifiers added at each step are made by taking new features, you need to make sure that you have enough variables to make the simple classifiers. So, if you took very few variables from your data to begin with, you will need to add other features for this part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost with decision tree of depth one\n",
    "Use decision tree of depth one as your base classifier. For Adaboost parameters use n_estimators = 5.  Again use the class_weight parameter for your decision tree classifier to deal with the unbalanced data. You may use the 'balanced' option.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.637030935827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "AdaBoost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1, class_weight = 'balanced'), n_estimators=5,\n",
    "                              learning_rate=1)\n",
    "AdaBoost.fit(X_train, y_train)\n",
    "score = AdaBoost.score(X_test, y_test)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you decrease or increase the number of your estimators?\n",
    "\n",
    "Try using n_estimators = { 1, 2, 5, 10} and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Random Forests\n",
    "\n",
    "Another approach for making stronger estimators from the basic ones is using random forests which provides a strong etimator built from a number of decision tree estimators. Each individual decision tree is made by using a random subset of the features. In addidion to that the training samples used for each tree are also random bootstrap samples from the training set (of the same size).\n",
    "\n",
    "In this part you will make a random forest classifier. Refer to\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "for the specifications and the parameters. You can use n_estimators and max_features to specify the number of the estimators and the number of features you want to use for building each tree. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exeriment with Random Forest \n",
    "\n",
    "Use n_estimator = 10 and max_features=sqrt(n_features). You may use max_depth = 20 in combination with min_samples_split = 2 to stop the trees from growing too deep. Again use the class_weight parameter for your decision tree classifier to deal with the unbalanced data.\n",
    "\n",
    "Define your classifier. Use fit and score functions to fit your model and compute the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rforest = RandomForestClassifier(n_estimators=2, max_features = 'sqrt', max_depth = 20, min_samples_split = 2 ,\n",
    "                                 class_weight = 'balanced')\n",
    "\n",
    "rforest.fit(X_train, y_train)\n",
    "score = rforest.score(X_test, y_test)\n",
    "\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you use other parameters?\n",
    "Use different values for n_estimator and max_depth. Report and compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
